Demo:
[https://www.loom.com/share/841c4c884ba846649be85dca0a7260cf?sid=5df52a55-a5cc-4c93-854c-cf0c8bbb3488](https://www.loom.com/share/9fef54b8501d4fd4a1ab3bffaf4379f6?sid=ee679848-c592-4c25-a8e8-145cc2514467)

```markdown
# PDF Knowledge Base Chat

This project provides a chat interface that lets you ask questions about the content of your PDF files. The application extracts text from PDFs, splits it into chunks, generates embeddings via the Mistral API, and uses a FAISS index to quickly retrieve relevant content based on your queries. The final answer is generated by a chat model that leverages both the PDF content and the retrieved context.

## Directory Structure


veershah0083-rag/
├── app.py
└── embeddings_cache.pkl


- **app.py**: The main application file that sets up the PDF knowledge base, processes PDFs, generates embeddings, and launches the Gradio chat interface.
- **embeddings_cache.pkl**: A cache file used to store embeddings of PDF text chunks for faster startups.

## Prerequisites

- Python 3.7 or later
- Required Python libraries:
  - [Gradio](https://gradio.app/)
  - [PyPDF2](https://pypi.org/project/PyPDF2/)
  - [FAISS](https://github.com/facebookresearch/faiss) (use `faiss-cpu` for most users)
  - [NumPy](https://numpy.org/)
  - [Mistralai](https://github.com/mistralai/mistralai) (or any equivalent package that provides the `MistralClient`)

## Installation

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/veershah0083-rag.git
   cd veershah0083-rag
   ```

2. **Install Required Packages**

   ```bash
   pip install gradio PyPDF2 faiss-cpu numpy mistralai
   ```

   > **Note:** If you encounter issues with `faiss-cpu`, please refer to the [FAISS installation instructions](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md).

## Setup

### 1. Configure the API Key

Open `app.py` and update the API key line:

```python
mistral_api_key = "MISTRAL_API_KEY"  # Replace with your actual API key
```

### 2. Prepare the Knowledge Base

- The application expects a directory named `knowledge_base` in the project root.
- If the directory doesn't exist, it will be created automatically on the first run.
- **Add your PDF files** into the `knowledge_base` directory. The application will process these files to build the knowledge base.

### 3. Manage the Embeddings Cache

- The file `embeddings_cache.pkl` stores the generated embeddings.
- When you add or update PDF files, delete this file to force regeneration of embeddings.

## Running the Application

To start the application, run:

```bash
python app.py
```

This command will launch a Gradio web interface in your default browser.

## Using the Application

- **Chat Interface**: Type your questions into the chat box. The application retrieves relevant text chunks from your PDFs, builds a prompt, and uses the Mistral model to generate a response.
- **Upload PDFs**: Use the file upload panel in the interface to add new PDF files. After uploading, restart the application to process the new files.
- **Knowledge Base Status**: The interface displays the current status of the knowledge base, including the number of text chunks loaded.

## Troubleshooting

- **No PDFs Detected**: Ensure that the `knowledge_base` directory exists and contains valid PDF files.
- **Cache Errors**: If there are issues with `embeddings_cache.pkl`, delete the file to force a rebuild of the embeddings.
- **API or Model Errors**: Verify that your Mistral API key is valid and that you have a stable internet connection if required by the API.

## Acknowledgements

- [Mistral API](https://www.mistral.ai/) for the chat model and embedding services.
- [Gradio](https://gradio.app/) for the interactive web interface.
- [FAISS](https://github.com/facebookresearch/faiss) for efficient similarity search.
- [PyPDF2](https://pypi.org/project/PyPDF2/) for PDF processing.
```
